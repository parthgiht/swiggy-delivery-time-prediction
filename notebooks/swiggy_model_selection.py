# -*- coding: utf-8 -*-
"""Swiggy_model_selection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v18wl6h-gUi7IpQ9cnGs9B8Sl4TLp7Cx
"""

import pandas as pd
import numpy as np

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer, KNNImputer, MissingIndicator
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, MinMaxScaler, PowerTransformer, OrdinalEncoder
from sklearn.model_selection import train_test_split

# Commented out IPython magic to ensure Python compatibility.
# %pip install mlflow dagshub

import dagshub
dagshub.init(repo_owner='parthgiht', repo_name='swiggy-delivery-time-prediction', mlflow=True)

import mlflow

# set the tracking server

mlflow.set_tracking_uri("https://dagshub.com/parthgiht/swiggy-delivery-time-prediction.mlflow")

# mlflow experiment

mlflow.set_experiment("Exp 2 - Model Selection")

from sklearn import set_config

set_config(transform_output="pandas")

"""## Load data"""

# load the data

Data = pd.read_csv('/content/swiggy_cleaned.csv')

Data.head()

Data.columns

Data['distance_type'] = pd.cut(
    Data['distance'],
    bins=[0, 5, 10, 15, 25],
    right=False,
    labels=['short', 'medium', 'long', 'very_long']
)

# drop columns not required for model input

columns_to_drop =  ['rider_id',
                    'restaurant_latitude',
                    'restaurant_longitude',
                    'delivery_latitude',
                    'delivery_longitude',
                    'order_date',
                    "order_time_hour",
                    "order_day",
                    "city_name",
                    "order_day_of_week",
                    "order_month"]

Data.drop(columns=columns_to_drop, inplace=True)

Data

# check for missing values

Data.isna().sum()

# check for duplicates

Data.duplicated().sum()

import missingno as msno

msno.matrix(Data)

# columns that have missing values

missing_cols = Data.isna().any(axis=0).loc[lambda x: x].index

missing_cols

"""## Drop missing values"""

temp_df = Data.copy().dropna()

# split into X and y

X = temp_df.drop(columns='time_taken')
y = temp_df['time_taken']

X

# train test split

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)

print("The size of train data is",X_train.shape)
print("The shape of test data is",X_test.shape)

# missing values in train data

X_train.isna().sum()

# transform target column

pt = PowerTransformer()

y_train_pt = pt.fit_transform(y_train.values.reshape(-1,1))
y_test_pt = pt.transform(y_test.values.reshape(-1,1))

missing_cols

# percentage of rows in data having missing values
X_train.isna().any(axis=1).mean().round(2) * 100

X_train.isna().sum()

"""## Pre processing pipeline"""

Num_cols = ["age","ratings","pickup_time_minutes","distance"]

Nominal_cat_cols = ['weather',
                    'type_of_order',
                    'type_of_vehicle',
                    "festival",
                    "city_type",
                    "is_weekend",
                    "order_time_of_day"]

Ordinal_cat_cols = ["traffic","distance_type"]

Nominal_cat_cols

# generate order for ordinal encoding

traffic_order = ["low","medium","high","jam"]

distance_type_order = ["short","medium","long","very_long"]

# unique categories the ordinal columns

for col in Ordinal_cat_cols:
    print(col,X_train[col].unique())

# build a preprocessor

preprocessor = ColumnTransformer(transformers=[
    ("scale", MinMaxScaler(), Num_cols),
    ("nominal_encode", OneHotEncoder(drop="first",handle_unknown="ignore",
                                     sparse_output=False), Nominal_cat_cols),
    ("ordinal_encode", OrdinalEncoder(categories=[traffic_order,distance_type_order],
                                      encoded_missing_value=-999,
                                      handle_unknown="use_encoded_value",
                                      unknown_value=-1), Ordinal_cat_cols)
],remainder="passthrough",n_jobs=-1,force_int_remainder_cols=False,verbose_feature_names_out=False)


preprocessor

# build the pipeline

processing_pipeline = Pipeline(steps=[
                                ("preprocess",preprocessor)
                            ])

processing_pipeline

# do data preprocessing

X_train_trans = processing_pipeline.fit_transform(X_train)

X_test_trans = processing_pipeline.transform(X_test)

X_train_trans

# Commented out IPython magic to ensure Python compatibility.
# %pip install optuna

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import r2_score, mean_absolute_error
from sklearn.svm import SVR
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
import optuna

def objective(trial):
    # choose model
    model_name = trial.suggest_categorical(
        "model",
        ["SupportVectorMachine","RandomForest","KNN","GradientBoosting","XGBoost","LightGBM"]
    )

    run_name = model_name  # base name, will append params later

    if model_name == "SupportVectorMachine":
        kernel_svm = trial.suggest_categorical("kernel_svm",["linear","poly","rbf"])
        run_name += f"_{kernel_svm}"

        if kernel_svm == "linear":
            c_linear = trial.suggest_float("c_linear",0,10)
            run_name += f"_C{c_linear:.2f}"
            model = SVR(C=c_linear,kernel="linear")

        elif kernel_svm == "poly":
            c_poly = trial.suggest_float("c_poly",0,10)
            degree_poly = trial.suggest_int("degree_poly",1,5)
            run_name += f"_C{c_poly:.2f}_deg{degree_poly}"
            model = SVR(C=c_poly,degree=degree_poly,kernel="poly")

        else:
            c_rbf = trial.suggest_float("c_rbf",0,100)
            gamma_rbf = trial.suggest_float("gamma_rbf",0,10)
            run_name += f"_C{c_rbf:.2f}_gamma{gamma_rbf:.2f}"
            model = SVR(C=c_rbf,gamma=gamma_rbf,kernel="rbf")

    elif model_name == "RandomForest":
        n_estimators_rf = trial.suggest_int("n_estimators_rf",10,200)
        max_depth_rf = trial.suggest_int("max_depth_rf",2,20)
        run_name += f"_n{n_estimators_rf}_d{max_depth_rf}"
        model = RandomForestRegressor(
            n_estimators=n_estimators_rf,
            max_depth=max_depth_rf,
            random_state=42,
            n_jobs=-1
        )

    elif model_name == "GradientBoosting":
        n_estimators_gb = trial.suggest_int("n_estimators_gb",10,200)
        learning_rate_gb = trial.suggest_float("learning_rate_gb",0,1)
        max_depth_gb = trial.suggest_int("max_depth_gb",2,20)
        run_name += f"_n{n_estimators_gb}_lr{learning_rate_gb:.2f}_d{max_depth_gb}"
        model = GradientBoostingRegressor(
            n_estimators=n_estimators_gb,
            learning_rate=learning_rate_gb,
            max_depth=max_depth_gb,
            random_state=42
        )

    elif model_name == "KNN":
        n_neighbors_knn = trial.suggest_int("n_neighbors_knn",1,25)
        weights_knn = trial.suggest_categorical("weights_knn",["uniform","distance"])
        run_name += f"_k{n_neighbors_knn}_{weights_knn}"
        model = KNeighborsRegressor(
            n_neighbors=n_neighbors_knn,
            weights=weights_knn,
            n_jobs=-1
        )

    elif model_name == "XGBoost":
        n_estimators_xgb = trial.suggest_int("n_estimators_xgb",10,200)
        learning_rate_xgb = trial.suggest_float("learning_rate_xgb",0.1,0.5)
        max_depth_xgb = trial.suggest_int("max_depth_xgb",2,20)
        run_name += f"_n{n_estimators_xgb}_lr{learning_rate_xgb:.2f}_d{max_depth_xgb}"
        model = XGBRegressor(
            n_estimators=n_estimators_xgb,
            learning_rate=learning_rate_xgb,
            max_depth=max_depth_xgb,
            random_state=42,
            n_jobs=-1
        )

    elif model_name == "LightGBM":
        n_estimators_lgbm = trial.suggest_int("n_estimators_lgbm",10,200)
        learning_rate_lgbm = trial.suggest_float("learning_rate_lgbm",0.1,0.5)
        max_depth_lgbm = trial.suggest_int("max_depth_lgbm",2,20)
        run_name += f"_n{n_estimators_lgbm}_lr{learning_rate_lgbm:.2f}_d{max_depth_lgbm}"
        model = LGBMRegressor(
            n_estimators=n_estimators_lgbm,
            learning_rate=learning_rate_lgbm,
            max_depth=max_depth_lgbm,
            random_state=42
        )

    # ----------------- MLflow run with custom run_name -------------------
    with mlflow.start_run(nested=True, run_name=run_name):
        # train
        model.fit(X_train_trans, y_train_pt.values.ravel())

        # log params
        mlflow.log_params(model.get_params())

        # predictions
        y_pred_test = model.predict(X_test_trans)
        y_pred_test_org = pt.inverse_transform(y_pred_test.reshape(-1,1))

        # error
        error = mean_absolute_error(y_test, y_pred_test_org)

        # log
        mlflow.log_param("model", model_name)
        mlflow.log_metric("MAE", error)

        return error

# create optuna study
study = optuna.create_study(direction="minimize",study_name="model_selection")

with mlflow.start_run(run_name="Best Model") as parent:
    # optimize the objective function
    study.optimize(objective,n_trials=30,n_jobs=-1)

    # log the best parameters
    mlflow.log_params(study.best_params)

    # log the best score
    mlflow.log_metric("best_score",study.best_value)

# best params
study.best_params

# best score
study.best_value

Lgbm_params = {
 'n_estimators': 190,
 'learning_rate': 0.10107246491072239,
 'max_depth': 20
}

# train the model on best parameters

lgbm = LGBMRegressor(**Lgbm_params)

lgbm.fit(X_train_trans,y_train_pt.values.ravel())

# get the predictions
y_pred_train = lgbm.predict(X_train_trans)
y_pred_test = lgbm.predict(X_test_trans)

# get the actual predictions values

y_pred_train_org = pt.inverse_transform(y_pred_train.reshape(-1,1))
y_pred_test_org = pt.inverse_transform(y_pred_test.reshape(-1,1))

from sklearn.metrics import mean_absolute_error, r2_score

print(f"The train error is {mean_absolute_error(y_train,y_pred_train_org):.2f} minutes")
print(f"The test error is {mean_absolute_error(y_test,y_pred_test_org):.2f} minutes")

print(f"The train r2 score is {r2_score(y_train,y_pred_train_org):.2f}")
print(f"The test r2 score is {r2_score(y_test,y_pred_test_org):.2f}")

# dataframe of results

study.trials_dataframe()

# model frequency

study.trials_dataframe()['params_model'].value_counts()

# avg scores for all tested models

study.trials_dataframe().groupby("params_model")['value'].mean().sort_values()

from sklearn.compose import TransformedTargetRegressor

model = TransformedTargetRegressor(regressor=lgbm,
                                    transformer=pt)

from sklearn.model_selection import cross_val_score

scores = cross_val_score(model,
                         X_train_trans,
                         y_train,
                         scoring="neg_mean_absolute_error",
                         cv=5,n_jobs=-1)

scores

# mean score

- scores.mean()

# optimization history plot

optuna.visualization.plot_optimization_history(study)

# partial coord plot

optuna.visualization.plot_parallel_coordinate(study,params=["model"])

