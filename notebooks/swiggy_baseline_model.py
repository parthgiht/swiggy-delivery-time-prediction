# -*- coding: utf-8 -*-
"""Swiggy_baseline_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_DZrITzF0zkTbvzubNmR3P9ZaKBpmkBG
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer, KNNImputer, MissingIndicator
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, OrdinalEncoder, MinMaxScaler, PowerTransformer
from sklearn.model_selection import train_test_split

from sklearn import set_config

set_config(transform_output="pandas")

"""## Load data"""

Data = pd.read_csv('/content/Swiggy_cleaned_data.csv')

Data.head()

"""## Clean data"""

Data.columns

# drop columns not required for model input

columns_to_drop =  ['rider_id',
                    'restaurant_latitude',
                    'restaurant_longitude',
                    'delivery_latitude',
                    'delivery_longitude',
                    'order_date',
                    "order_time_hour",
                    "order_day"]

Data.drop(columns=columns_to_drop, inplace=True)
Data

# check for missing values

Data.isna().sum()

# check for duplicates

Data.duplicated().sum()

# add new column (distance_type)
def distance_type(Data: pd.DataFrame):
  Data = Data.assign(distance_type = pd.cut(Data['distance'], bins = [0, 5, 10, 15, 25,],
                                            right = False, labels = ['short', 'medium', 'long', 'very_long']))
  return Data

Data = distance_type(Data)

Data.head()

import missingno as msno

msno.matrix(Data)

# columns that have missing values

missing_cols = Data.isna().any(axis=0).loc[lambda x: x].index

missing_cols



"""## Data preprocessing"""

temp_df = Data.copy().dropna()

# Seperate independent and dependent features
X = temp_df.drop(columns = 'time_taken')
y = temp_df['time_taken']

X

y

X.shape

y.shape

# Train test split the model
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

print("The size of train data is",X_train.shape)
print("The shape of test data is",X_test.shape)

X_train.shape

y_train.shape

# missing data in training data

X_train.isna().sum()

X_train.columns

len(X_train.columns)

# do basic preprocessing

Num_cols = ["age","ratings","pickup_time_minutes","distance"]

Nominal_cat_cols = ['weather','type_of_order',
                    'type_of_vehicle',"festival",
                    "city_type","city_name","order_month",
                    "order_day_of_week",
                    "is_weekend",
                    "order_time_of_day"]

Ordinal_cat_cols = ["traffic","distance_type"]

len(Num_cols + Nominal_cat_cols + Ordinal_cat_cols)

for col in Ordinal_cat_cols:
  print(col, X_train[col].unique())

# generate order for ordinal encoding

traffic_order = ["low","medium","high","jam"]

distance_type_order = ["short","medium","long","very_long"]

# build a preprocessor

preprocessor = ColumnTransformer(transformers = [
    ("scale", MinMaxScaler(), Num_cols),
    ("nominal_encode", OneHotEncoder(drop = "first", handle_unknown = "ignore", sparse_output = False), Nominal_cat_cols),
    ("ordinal_encode", OrdinalEncoder(categories = [traffic_order, distance_type_order]), Ordinal_cat_cols),
],remainder="passthrough",n_jobs=-1,force_int_remainder_cols=False,verbose_feature_names_out=False)

preprocessor.set_output(transform="pandas")

# Transform the data
X_train_transformed = preprocessor.fit_transform(X_train)
X_test_transformed = preprocessor.transform(X_test)

X_train_transformed

# transform target column

pt = PowerTransformer()

y_train_pt = pt.fit_transform(y_train.values.reshape(-1,1))
y_test_pt = pt.transform(y_test.values.reshape(-1,1))

pt.lambdas_

y_train_pt



"""## Train initial baseline model"""

from sklearn.linear_model import LinearRegression

lr = LinearRegression()

lr.fit(X_train_transformed,y_train_pt)

# get the predictions
y_pred_train = lr.predict(X_train_transformed)
y_pred_test = lr.predict(X_test_transformed)

# get the actual predictions values

y_pred_train_org = pt.inverse_transform(y_pred_train.reshape(-1,1))
y_pred_test_org = pt.inverse_transform(y_pred_test.reshape(-1,1))

from sklearn.metrics import mean_absolute_error, r2_score

print(f"The train error is {mean_absolute_error(y_train,y_pred_train_org):.2f} minutes")
print(f"The test error is {mean_absolute_error(y_test,y_pred_test_org):.2f} minutes")

print(f"The train r2 score is {r2_score(y_train,y_pred_train_org):.2f}")
print(f"The test r2 score is {r2_score(y_test,y_pred_test_org):.2f}")



"""## Impute missing values"""

temp_df = Data.copy()

# split into X and y

X = temp_df.drop(columns='time_taken')
y = temp_df['time_taken']

X

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

X_train.shape

X_test.shape

y_train.shape

y_test.shape

# Missing values in train data
X_train.isna().sum()

# Transform target column

pt = PowerTransformer()

y_train_pt = pt.fit_transform(y_train.values.reshape(-1,1))
y_test_pt = pt.transform(y_test.values.reshape(-1,1))

missing_cols

# percentage of rows in data having missing values

print(X_train.isna().any(axis=1).mean().round(2) * 100)



"""## Age"""

X_train['age'].describe()

# missing values in the column

X_train['age'].isna().sum()

# median value

age_median = X_train['age'].median()

age_median

# plot the kde plot

sns.kdeplot(X_train['age'],label="original")
sns.kdeplot(X_train['age'].fillna(age_median),label="imputed")
plt.legend()

"""**Observation**:

1. Changed the distribution of the age column.
2. Use Advanced imputation techniques like KNN imputer.
"""



"""## Ratings"""

X_train['ratings'].describe()

X_train['ratings'].isna().sum()

# avg rating

ratings_mean = X_train['ratings'].mean()

ratings_mean

# fill and plot kdeplot

sns.kdeplot(X_train['ratings'],label="original")
sns.kdeplot(X_train['ratings'].fillna(ratings_mean),label="imputed")
plt.legend()



"""## Weather"""

X_train['weather'].value_counts()

X_train['weather'].isna().sum()

sns.countplot(X_train['weather'])

# capture the missingness

missing_weather = MissingIndicator()
missing_weather.set_output(transform="pandas")

pd.concat([X_train['weather'],missing_weather.fit_transform(X_train[['weather']])],axis=1).sample(50)



"""## Traffic"""

X_train['traffic'].value_counts()

X_train['traffic'].isna().sum()

sns.countplot(X_train['traffic'])



"""## Multiple Deliveries"""

# value counts

X_train['multiple_deliveries'].value_counts()

# countplot

sns.countplot(X_train['multiple_deliveries'].apply(str))

# number of missing values

X_train['multiple_deliveries'].isna().sum()

# mode value

multiple_deliveries_mode = X_train['multiple_deliveries'].mode()[0]
multiple_deliveries_mode

# fill na values with mode

sns.countplot(X_train['multiple_deliveries'].fillna(multiple_deliveries_mode).apply(str))



"""## Festival"""

X_train['festival'].value_counts()

sns.countplot(X_train['festival'])

X_train['festival'].isna().sum()

# mode value

festival_mode = X_train['festival'].mode()[0]

# fill with mode

sns.countplot(X_train['festival'].fillna(festival_mode))



"""## City type"""

X_train['city_type'].value_counts()

# number of missing values

X_train['city_type'].isna().sum()

# countplot

sns.countplot(X_train['city_type'])

# mode value

city_type_mode = X_train['city_type'].mode()[0]
city_type_mode

# fill with mode

sns.countplot(X_train['city_type'].fillna(city_type_mode))



"""## Pickup time minutes"""

X_train['pickup_time_minutes'].describe()

# missing values in the column

X_train['pickup_time_minutes'].isna().sum()

# median value

pickup_time_minutes_median = X_train['pickup_time_minutes'].median()
pickup_time_minutes_median

# histplot

sns.histplot(X_train['pickup_time_minutes'],kde=True,label='original')
sns.histplot(X_train['pickup_time_minutes'].fillna(pickup_time_minutes_median),kde=True,label='imputed')
plt.legend()



"""## Order time of day"""

# value counts

X_train['order_time_of_day'].value_counts()

X_train['order_time_of_day'].isna().sum()

# countplot

sns.countplot(X_train['order_time_of_day'])

# rows where the data is missing

X_train[X_train['order_time_of_day'].isna()]



"""## Distance"""

X_train['distance'].describe()

# number of missing values

X_train['distance'].isna().sum()

# avg distance

distance_mean = X_train['distance'].mean()
distance_mean

# kdeplot

sns.kdeplot(X_train['distance'],label='original')
sns.kdeplot(X_train['distance'].fillna(distance_mean),label='imputed')
plt.legend()



"""## Distance type"""

# value counts

X_train['distance_type'].value_counts()

# missing values

X_train['distance_type'].isna().sum()

# countplot

sns.countplot(X_train['distance_type'])



"""## Imputation Pipeline"""

Nominal_cat_cols

X_train.isna().sum()

# features to fill values with mode

features_to_fill_mode = ['multiple_deliveries','festival','city_type']
features_to_fill_missing = [col for col in Nominal_cat_cols if col not in features_to_fill_mode]

features_to_fill_missing

# simple imputer to fill categorical vars with mode

simple_imputer = ColumnTransformer(transformers=[
    ("mode_imputer",SimpleImputer(strategy="most_frequent"),features_to_fill_mode),
    ("missing_imputer",SimpleImputer(strategy="constant",fill_value="missing"),features_to_fill_missing)
],remainder="passthrough",n_jobs=-1,force_int_remainder_cols=False,verbose_feature_names_out=False)

simple_imputer

simple_imputer.fit_transform(X_train)

simple_imputer.fit_transform(X_train).isna().sum()

# KNN imputer

KNN_imputer = KNNImputer(n_neighbors = 5)
KNN_imputer

# do basic preprocessing

Num_cols = ["age","ratings","pickup_time_minutes","distance"]

Nominal_cat_cols = ['weather','type_of_order',
                    'type_of_vehicle',"festival",
                    "city_type","city_name","order_month",
                    "order_day_of_week",
                    "is_weekend",
                    "order_time_of_day"]

Ordinal_cat_cols = ["traffic","distance_type"]

# generate order for ordinal encoding

traffic_order = ["low","medium","high","jam"]

distance_type_order = ["short","medium","long","very_long"]

# unique categories the ordinal columns

for col in Ordinal_cat_cols:
    print(col,X_train[col].unique())

# build a preprocessor

preprocessor = ColumnTransformer(transformers=[
    ("scale", MinMaxScaler(), Num_cols),
    ("nominal_encode", OneHotEncoder(drop="first",handle_unknown="ignore",
                                     sparse_output=False), Nominal_cat_cols),
    ("ordinal_encode", OrdinalEncoder(categories=[traffic_order,distance_type_order],
                                      encoded_missing_value=-999,
                                      handle_unknown="use_encoded_value",
                                      unknown_value=-1), Ordinal_cat_cols)
],remainder="passthrough",n_jobs=-1,force_int_remainder_cols=False,verbose_feature_names_out=False)


preprocessor

preprocessor.fit_transform(X_train)

preprocessor.fit_transform(X_train).isna().sum().loc[lambda ser : ser.ge(1)]

# build the pipeline

processing_pipeline = Pipeline(steps=[
                                ("simple_imputer",simple_imputer),
                                ("preprocess",preprocessor),
                                ("knn_imputer",KNN_imputer)
                            ])

processing_pipeline

# fit and transform the pipeline on X_train

processing_pipeline.fit_transform(X_train)

"""## Linear Regression"""

from sklearn.linear_model import LinearRegression

lr = LinearRegression()

model_pipe = Pipeline(steps=[
                                ("preprocessing",processing_pipeline),
                                ("model",lr)
                            ])

model_pipe

# fit the pipeline on data

model_pipe.fit(X_train,y_train_pt)

# get the predictions
y_pred_train = model_pipe.predict(X_train)
y_pred_test = model_pipe.predict(X_test)

# get the actual predictions values

y_pred_train_org = pt.inverse_transform(y_pred_train.reshape(-1,1))
y_pred_test_org = pt.inverse_transform(y_pred_test.reshape(-1,1))

from sklearn.metrics import mean_absolute_error, r2_score

print(f"The train error is {mean_absolute_error(y_train,y_pred_train_org):.2f} minutes")
print(f"The test error is {mean_absolute_error(y_test,y_pred_test_org):.2f} minutes")

print(f"The train r2 score is {r2_score(y_train,y_pred_train_org):.2f}")
print(f"The test r2 score is {r2_score(y_test,y_pred_test_org):.2f}")

"""## RandomForest"""

from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(random_state=42,n_jobs=-1)

model_pipe = Pipeline(steps=[
                                ("preprocessing",processing_pipeline),
                                ("model",rf)
                            ])

model_pipe

# fit the pipeline on data

model_pipe.fit(X_train,y_train_pt.values.ravel())

# get the predictions
y_pred_train = model_pipe.predict(X_train)
y_pred_test = model_pipe.predict(X_test)

# get the actual predictions values

y_pred_train_org = pt.inverse_transform(y_pred_train.reshape(-1,1))
y_pred_test_org = pt.inverse_transform(y_pred_test.reshape(-1,1))

from sklearn.metrics import mean_absolute_error, r2_score

print(f"The train error is {mean_absolute_error(y_train,y_pred_train_org):.2f} minutes")
print(f"The test error is {mean_absolute_error(y_test,y_pred_test_org):.2f} minutes")

print(f"The train r2 score is {r2_score(y_train,y_pred_train_org):.2f}")
print(f"The test r2 score is {r2_score(y_test,y_pred_test_org):.2f}")

