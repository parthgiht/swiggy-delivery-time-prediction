# -*- coding: utf-8 -*-
"""Swiggy_stacking_regressor_Hp-tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vJgF7qKARJIougijgV6wezOfUbvWg4St
"""

import numpy as np
import pandas as pd

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, MinMaxScaler, PowerTransformer, OrdinalEncoder

from sklearn.model_selection import train_test_split

# Commented out IPython magic to ensure Python compatibility.
# %pip install mlflow dagshub

import dagshub
dagshub.init(repo_owner='parthgiht', repo_name='swiggy-delivery-time-prediction', mlflow=True)

import mlflow

# set the tracking server

mlflow.set_tracking_uri("https://dagshub.com/parthgiht/swiggy-delivery-time-prediction.mlflow")

# mlflow experiment

mlflow.set_experiment("Exp 5 - Stacking Regressor")

from sklearn import set_config

set_config(transform_output="pandas")

# load the data

Data = pd.read_csv('/content/swiggy_cleaned.csv')

Data.head()

Data['distance_type'] = pd.cut(
    Data['distance'],
    bins=[0, 5, 10, 15, 25],
    right=False,
    labels=['short', 'medium', 'long', 'very_long']
)

Data.columns

# drop columns not required for model input

columns_to_drop =  ['rider_id',
                    'restaurant_latitude',
                    'restaurant_longitude',
                    'delivery_latitude',
                    'delivery_longitude',
                    'order_date',
                    "order_time_hour",
                    "order_day",
                    "city_name",
                    "order_day_of_week",
                    "order_month"]

Data.drop(columns=columns_to_drop, inplace=True)

Data

# check for missing values

Data.isna().sum()

# check for duplicates

Data.duplicated().sum()

import missingno as msno

msno.matrix(Data)

# columns that have missing values

missing_cols = Data.isna().any(axis=0).loc[lambda x: x].index

missing_cols

"""## Drop missing values"""

temp_df = Data.copy().dropna()

# split into X and y

X = temp_df.drop(columns='time_taken')
y = temp_df['time_taken']

X

# train test split

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)

print("The size of train data is",X_train.shape)
print("The shape of test data is",X_test.shape)

# missing values in train data

X_train.isna().sum()

# transform target column

pt = PowerTransformer()

y_train_pt = pt.fit_transform(y_train.values.reshape(-1,1))
y_test_pt = pt.transform(y_test.values.reshape(-1,1))

missing_cols

# percentage of rows in data having missing values

X_train .isna().any(axis=1).mean().round(2) * 100

"""## Pre-Processing Pipeline"""

Num_cols = ["age","ratings","pickup_time_minutes","distance"]

Nominal_cat_cols = ['weather',
                    'type_of_order',
                    'type_of_vehicle',
                    "festival",
                    "city_type",
                    "is_weekend",
                    "order_time_of_day"]

Ordinal_cat_cols = ["traffic","distance_type"]

# generate order for ordinal encoding

traffic_order = ["low","medium","high","jam"]

distance_type_order = ["short","medium","long","very_long"]

# build a preprocessor

preprocessor = ColumnTransformer(transformers=[
    ("scale", MinMaxScaler(), Num_cols),
    ("nominal_encode", OneHotEncoder(drop="first",handle_unknown="ignore",
                                     sparse_output=False), Nominal_cat_cols),
    ("ordinal_encode", OrdinalEncoder(categories=[traffic_order,distance_type_order],
                                      encoded_missing_value=-999,
                                      handle_unknown="use_encoded_value",
                                      unknown_value=-1), Ordinal_cat_cols)
],remainder="passthrough",n_jobs=-1,force_int_remainder_cols=False,verbose_feature_names_out=False)


preprocessor

# build the pipeline

processing_pipeline = Pipeline(steps=[
                                ("preprocess",preprocessor)
                            ])

processing_pipeline

# do data preprocessing

X_train_trans = processing_pipeline.fit_transform(X_train)

X_test_trans = processing_pipeline.transform(X_test)

# Commented out IPython magic to ensure Python compatibility.
# %pip install optuna

from lightgbm import LGBMRegressor
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, StackingRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.compose import TransformedTargetRegressor
from sklearn.metrics import r2_score, mean_absolute_error
import optuna

# Build the best model
best_rf_params = {
    'n_estimators' : 347,
    'criterion' : 'squared_error',
    'max_depth' : 25,
    'max_features' : None,
    'min_samples_split' : 4,
    'min_samples_leaf' : 6,
    'max_samples' : 0.6919341182308667 }

best_lgbm_params = {
    'n_estimators' : 61,
    'learning_rate' : 0.49484528540262995,
    'subsample' : 0.6349609818535225,
    'min_child_weight' : 14,
    'min_split_gain' : 0.003788774515052218,
    'reg_lambda' : 56.3178028449038 }



best_rf = RandomForestRegressor(**best_rf_params)
best_lgbm = LGBMRegressor(**best_lgbm_params)

def objective(trial):
  with mlflow.start_run(nested = True):
    meta_model_name = trial.suggest_categorical("model", ["LR","KNN", "DI"])

    if meta_model_name == "LR":
      meta = LinearRegression()

    elif meta_model_name == "KNN":
      n_neighbors_knn = trial.suggest_int("n_neighbors_knn", 1, 15)
      weights_knn = trial.suggest_categorical("weights_knn", ["uniform", "distance"])
      meta = KNeighborsRegressor(n_neighbors = n_neighbors_knn, weights = weights_knn, n_jobs = -1)

    elif meta_model_name == "DI":
      max_depth_dt = trial.suggest_int("max_depth", 1, 10)
      min_samples_split_dt = trial.suggest_int("min_samples_split_dt", 2, 10)
      min_samples_leaf_dt = trial.suggest_int("min_samples_leaf_dt", 1, 10)

      meta = DecisionTreeRegressor(max_depth = max_depth_dt,
                                  min_samples_split = min_samples_split_dt,
                                  min_samples_leaf = min_samples_leaf_dt,
                                  random_state = 42)


    # Log meta model name
    mlflow.log_param("meta_model_name", meta_model_name)

    # Stacking regressor
    stacking_reg = StackingRegressor(estimators = [("rf", best_rf),
                                                  ("lgbm", best_lgbm)],\
                                    final_estimator = meta,
                                    cv = 5, n_jobs = -1)
    # Build transformed regressor
    model = TransformedTargetRegressor(regressor = stacking_reg,
                                      transformer = pt)

    # train the model
    model.fit(X_train_trans, y_train)

    # get the predictions
    y_pred_test = model.predict(X_test_trans)

    # mean absolute error
    error = mean_absolute_error(y_test, y_pred_test)

    # log error
    mlflow.log_metric("MAE", error)

    return error

# Create optuna study
study = optuna.create_study(direction = "minimize")

with mlflow.start_run(run_name = "best_model"):
  # optimize the objective function
  study.optimize(objective, n_trials = 20, n_jobs = -1, show_progress_bar = True)

  # log the best parameters
  mlflow.log_params(study.best_params)

  # log the best score
  mlflow.log_metric("best_score", study.best_value)

best_params = study.best_params
best_params

study.trials_dataframe()['params_model'].value_counts()

study.trials_dataframe().groupby(by = 'params_model')['value'].mean().sort_values()

# Best score
study.best_value

# Optimization history plot

optuna.visualization.plot_optimization_history(study)

# Optimization history plot

optuna.visualization.plot_parallel_coordinate(study, params = ["model"])

