# -*- coding: utf-8 -*-
"""Swiggy_data_cleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ju1-hNOEcdLtSyqb1Wh0KMQYaQuyI-V3
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno

# Load the data
Data = pd.read_csv('/content/swiggy.csv')

Data.head()

Data.shape

Data.info()



"""## Handling Missing values"""

Data[Data == "NaN "]

Data.loc[43317, "Delivery_person_Ratings"]

# Strings NaN values in the data
print((Data == "NaN ").sum().sum())

# Columns wise strings NaN values
(Data == "NaN ").sum()

"""- Its a good point that target column not have any kind of missing values."""

# Replace string (NaN ) values with the simple (NaN)
Data.replace("NaN ", np.nan)

# Verify missing value in the data
Data.replace("NaN ",np.nan).isna().sum().sum()

"""#### NaN values in the weather column"""

print(Data.loc[:,"Weatherconditions"].str.replace("conditions ", "").replace("NaN", np.nan).isna().sum())

"""- 616 are the missing values in the weather conditions column"""

# Replace all the missing values from data
missing_data = Data.replace("NaN ",np.nan).assign(Weatherconditions = lambda Data_:(Data_['Weatherconditions'].str.replace("conditions ","").replace("NaN", np.nan)))

missing_data

# Finding original missing values which we have replace by (NaN ) --> (NaN)
missing_data.isna().sum()

# Overall missing vlaues in data
print(missing_data.isna().sum().sum())

"""### Missing values analysis

#### Matrix
"""

msno.matrix(missing_data)

# Bar plots for columns having missing values
msno.bar(missing_data)



"""### Missing corr"""

# Missing data correlation
msno.heatmap(missing_data)

"""**Observations**:-
1. The delivery person column are correlated to each other. This means that missingness in these column is highly realted which means lack of rider data.
2. The time ordered column is also related to the rider which shows that the data might be missing due to some network error where the system was unable to log rider details and time of order.
3. There is very high correlation between weather patterns and the road traffic information. Be clear that this does not mean that the road traffic and weather column are correlated. High corr in missingness means that if value in one columns is missing, it is high chance that it will be missing others as well.
4. Road traffic density also shows correlation to the rider as it might be provided through the phone of rider (need to investigate).
"""

missing_data[['Weatherconditions', 'Road_traffic_density']].isna().sum()

# Proving point of missingness
missing_data[['Weatherconditions', 'Road_traffic_density']].isna().all(axis = 1).sum()/ missing_data[['Weatherconditions', 'Road_traffic_density']].isna().sum()

"""### Dendrogram"""

# dendrogram of missingness
msno.dendrogram(missing_data)

"""- To understand the hierarchical relation between the columns"""

# Percentage of rows having missing vlaues in the data
print(round((missing_data.isna().any(axis = 1).sum()/missing_data.shape[0])*100,3))

"""- About 9% of the rows in data have missing values."""



"""## Basic Data Cleaning"""

Data.columns.tolist()

# Update the column names
def update_column_names(data: pd.DataFrame):
    data = data.rename({
        'ID': 'id',
        'Delivery_person_ID': 'rider_id',
        'Delivery_person_Age': 'age',
        'Delivery_person_Ratings': 'ratings',
        'Restaurant_latitude':'restaurant_latitude',
        'Restaurant_longitude':'restaurant_longitude',
        'Delivery_location_latitude': 'delivery_latitude',
        'Delivery_location_longitude': 'delivery_longitude',
        'Order_Date': 'order_date',
        'Time_Orderd': 'order_time',
        'Time_Order_picked': 'order_picked_time',
        'Weatherconditions': 'weather',
        'Road_traffic_density': 'traffic',
        'Vehicle_condition': 'vehicle_condition',
        'Type_of_order': 'type_of_order',
        'Type_of_vehicle': 'type_of_vehicle',
        'multiple_deliveries': 'multiple_deliveries',
        'Festival': 'festival',
        'City': 'city_type',
        'Time_taken(min)': 'time_taken',
    }, axis=1)

    return data

# Check the result
Data = update_column_names(Data)

Data.head()

# Check for duplicates values
print(Data.duplicated().sum())

"""## Column wise cleaning

### id
"""

Data['id'].nunique()

"""- All the ID values are unique.

### rider_id
"""

Data['rider_id'].nunique()

"""- There are 1320 unique rider id's in column."""

Data['rider_id'].value_counts()

# Extract city names from "Rider_id" column
Data['rider_id'].str.split("RES").str.get(0).rename("City_name")

"""## age"""

Data['age'].astype(float).describe()

# Box plot on age column
sns.boxplot(Data['age'].astype(float))

# Fetch those rows where rider age was 18(minor)
minors_data = Data.loc[Data['age'].astype(float) < 18]
minors_data

"""**Observations:-**
1. The star ratings of all the minor riders is 1.
2. The vehicle condition of the minor riders is very bad.
3. No weather and traffic conditions are available.
4. Age of all this minor raider are 15 which means below the permissable age to drive vehicle.
5. Latitude and longitude values in negative which is not possible. India is suited above the equater so, all latitude should be positive and east of meridian line so, longitude are positive as well.




**It seems like that removing this data for now make more sense than fixing it because a lot of data is missing.**

"""

minor_index = minors_data.index.tolist()
len(minor_index)



"""## ratings"""

Data['ratings'].astype(float).describe()

"""- 6 start rating may be a data error"""

sns.boxplot(Data['ratings'].astype(float))

# Rows where the ratings have 6 values
six_star = Data.loc[Data['ratings'] == "6"]
len(six_star)

six_star.head()

six_star_index = six_star.index.tolist()
len(six_star_index)



"""## location"""

# Fetch all the location related column from the data
location_columns = Data.columns[4:8].tolist()
location_columns

location_subset = Data.loc[:, location_columns ]
location_subset

location_subset.describe()

# Set the lowe bound limits for the lat and log
lower_bound_lat = 6.44
lower_bound_long = 68.70

# Fecth data on lat and long based on lower bound of lat and long
Data.loc[
    (Data["restaurant_latitude"] < lower_bound_lat) |
    (Data["restaurant_longitude"] < lower_bound_long) |
    (Data["delivery_latitude"] < lower_bound_lat) |
    (Data["delivery_longitude"] < lower_bound_long)
].sample(50)

# No. of rows in data where lat long errornous
location_subset.loc[
    (location_subset["restaurant_latitude"] < lower_bound_lat) |
    (location_subset["restaurant_longitude"] < lower_bound_long) |
    (location_subset["delivery_latitude"] < lower_bound_lat) |
    (location_subset["delivery_longitude"] < lower_bound_long)
].shape[0]

location_subset.loc[
    (location_subset["restaurant_latitude"] < lower_bound_lat) |
    (location_subset["restaurant_longitude"] < lower_bound_long) |
    (location_subset["delivery_latitude"] < lower_bound_lat) |
    (location_subset["delivery_longitude"] < lower_bound_long)
].describe()

location_subset.loc[
    (location_subset["restaurant_latitude"] < lower_bound_lat) |
    (location_subset["restaurant_longitude"] < lower_bound_long) |
    (location_subset["delivery_latitude"] < lower_bound_lat) |
    (location_subset["delivery_longitude"] < lower_bound_long)
].plot(kind = 'box')

plt.xticks(rotation = 45)

# Taking absolute values
location_subset.abs().plot(kind = 'box')

ax = plt.gca()
ax.set_xticklabels(ax.get_xticklabels(), rotation = 45)

# No. of rows after taking absolute values
location_subset.abs().loc[lambda Data_:
 (Data_['restaurant_latitude'] < lower_bound_lat) |
  (Data_['restaurant_longitude'] < lower_bound_long) |
  (Data_['delivery_latitude'] < lower_bound_lat) |
  (Data_['delivery_longitude'] < lower_bound_long)].shape[0]

# Lat long values less than 1
location_subset.abs().loc[lambda Data_:
 (Data_['restaurant_latitude'] < 1) |
  (Data_['restaurant_longitude'] < 1) |
  (Data_['delivery_latitude'] < 1) |
  (Data_['delivery_longitude'] < 1)]

def clean_lat_long(data: pd.DataFrame, threshold = 1):
  location_columns = location_subset.columns.tolist()
  return (
      data
      .assign(**{
          col: (
              np.where(data[col] < threshold, np.nan, data[col].values)
          )
          for col in location_columns
      })
  )

clean_lat_long(Data).isna().sum()



"""## order_date"""

Data['order_date'].isna().sum()

Data['order_date'].unique()

# Data range
order_date = pd.to_datetime(Data['order_date'], dayfirst = True)
order_date.max() - order_date.min()

# Min and Max dates
order_date.agg(["min","max"]).set_axis(["start","end"], axis = 0)

# Fetch day, day name, month, year
def extract_datetime_features(series):
    # Convert to datetime
    dates = pd.to_datetime(series, dayfirst=True)

    # Create an empty DataFrame
    df = pd.DataFrame()

    # Add features one by one
    df["day"] = dates.dt.day
    df["month"] = dates.dt.month
    df["year"] = dates.dt.year
    df["day_of_week"] = dates.dt.day_name()
    df["is_weekend"] = dates.dt.weekday.isin([5, 6]).astype(int)

    return df

extract_datetime_features(Data['order_date'])



"""## order time and order picked time"""

order_time_hr = pd.to_datetime(Data.replace("NaN ",np.nan)['order_time'], format = 'mixed').dt.hour

order_time_hr

def time_of_day(ser) :
  time_col = pd.to_datetime(ser, format = 'mixed').dt.hour

  return np.select(condlist = [(ser.between(6, 12, inclusive = 'left')),
                               (ser.between(12, 17, inclusive = 'left')),
                               (ser.between(17, 20, inclusive = 'left')),
                               (ser.between(20, 24, inclusive = 'left'))],
                  choicelist = ["morning", "afternoon", "evening", "night"],
                  default = "after_midnight")

time_subset = Data.loc[:,["order_time", "order_picked_time"]]
time_subset

time_subset.columns.tolist()

time_subset.assign(**{
        col: pd.to_datetime(time_subset[col].replace("NaN ", np.nan).dropna(),format='mixed')
        for col in time_subset.columns.tolist()}).assign(
        picked_time=lambda x: (x['order_picked_time'] - x['order_time']).dt.seconds / 60,
        order_time_hour=lambda x: x['order_time'].dt.hour,
        order_time_day=lambda x: x['order_time_hour'].pipe(time_of_day)).drop(columns=['order_time', 'order_picked_time'])



"""## weather"""

Data['weather'].value_counts()

# Unique values
Data['weather'].unique()

# Remove conditions from values
Data['weather'] = Data['weather'].str.replace("conditions","").str.lstrip()
Data['weather']

Data['weather'].replace("NaN", np.nan).unique()

"""## traffic"""

Data['traffic'].value_counts()

Data['traffic'].replace("NaN",np.nan).str.rstrip()

Data['traffic'].replace('NaN',np.nan).unique()



"""## vehicle condition"""

Data.columns

Data['vehicle_condition'].value_counts()



"""## type of order"""

Data['type_of_order'].value_counts()

Data['type_of_order'].str.rstrip().unique()



"""## type of vehicle"""

Data['type_of_vehicle'].value_counts()

Data['type_of_vehicle'].str.rstrip().unique()



"""## multiple deliveries"""

Data['multiple_deliveries'].value_counts()

Data['multiple_deliveries'].replace("NaN ",np.nan).astype(float).unique()



"""## festival"""

Data['festival'].value_counts()

Data['festival'].unique()

Data['festival'].replace("NaN ",np.nan).str.rstrip().unique()



"""## city type"""

Data['city_type'].value_counts()

Data['city_type'].replace("NaN ",np.nan).str.rstrip().unique()



"""## time taken"""

Data['time_taken'].str.replace("(min) ","").astype(int)



Data.columns

"""## Whole Function of cleaning data"""

# Assuming time_of_day is defined elsewhere as a function that can be piped to a Series

def data_cleaning(Data: pd.DataFrame):
    Data = Data.drop(columns='id')

    Data = Data.replace("NaN ", np.nan)

    minor_index = Data[Data['age'].astype(float) < 18].index
    six_star_index = Data[Data['ratings'].astype(float) > 5].index

    Data = Data.drop(index=minor_index)
    Data = Data.drop(index=six_star_index)

    Data['city_name'] = Data['rider_id'].str.split("RES").str.get(0)
    Data['age'] = Data['age'].astype(float)
    Data['ratings'] = Data['ratings'].astype(float)
    Data['restaurant_latitude'] = Data['restaurant_latitude'].abs()
    Data['restaurant_longitude'] = Data['restaurant_longitude'].abs()
    Data['delivery_latitude'] = Data['delivery_latitude'].abs()
    Data['delivery_longitude'] = Data['delivery_longitude'].abs()
    Data['order_date'] = pd.to_datetime(Data['order_date'], dayfirst=True)
    Data['order_day'] = Data['order_date'].dt.day
    Data['order_month'] = Data['order_date'].dt.month
    Data['order_day_of_week'] = Data['order_date'].dt.day_name().str.lower()
    Data['is_weekend'] = (Data['order_date'].dt.day_name().isin(["Saturday", "Sunday"])).astype(int)
    Data['order_time'] = pd.to_datetime(Data['order_time'], format='mixed')
    Data['order_picked_time'] = pd.to_datetime(Data['order_picked_time'], format='mixed')
    Data['pickup_time_minutes'] = (Data['order_picked_time'] - Data['order_time']).dt.seconds / 60
    Data['order_time_hour'] = Data['order_time'].dt.hour
    Data['order_time_of_day'] = Data['order_time_hour'].pipe(time_of_day)
    Data['weather'] = Data['weather'].str.replace("conditions ", "").str.lower().replace("nan", np.nan)
    Data['traffic'] = Data['traffic'].str.rstrip().str.lower()
    Data['type_of_order'] = Data['type_of_order'].str.rstrip().str.lower()
    Data['type_of_vehicle'] = Data['type_of_vehicle'].str.rstrip().str.lower()
    Data['festival'] = Data['festival'].str.rstrip().str.lower()
    Data['city_type'] = Data['city_type'].str.rstrip().str.lower()
    Data['multiple_deliveries'] = Data['multiple_deliveries'].astype(float)
    Data['time_taken'] = Data['time_taken'].str.replace("(min) ", "").astype(int)

    Data = Data.drop(columns=["order_time", "order_picked_time"])

    return Data

# Apply function on original data
data_cleaning(Data)

Data.columns

location_subset.columns.tolist()

"""### Finding distance"""

def calculate_haversine_distance(df):
  location_columns = location_subset.columns.tolist()
  lat1 = df[location_columns[0]]
  lon1 = df[location_columns[1]]
  lat2 = df[location_columns[2]]
  lon2 = df[location_columns[3]]

  lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])

  distance_lat = lat2 - lat1
  distance_lon = lon2 - lon1

  a = np.sin(
      distance_lat / 2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(distance_lon / 2.0)**2

  c = 2 * np.arcsin(np.sqrt(a))
  distance = 6371 * c

  return df.assign(distance = distance)

# add more data cleaning steps

cleaned_data = (
                Data.pipe(data_cleaning)
                .pipe(clean_lat_long)
                .pipe(calculate_haversine_distance)
                )

cleaned_data



"""## Validate data cleaning"""

# age range
cleaned_data['age'].agg(['min','max'])

# ratings range
cleaned_data['ratings'].agg(['min','max'])

# location columns
cleaned_data[location_columns].plot(kind = 'box')

plt.xticks(rotation = 45)

cleaned_data[location_columns].describe()

# values in categorical columns
cat_cols = cleaned_data.select_dtypes(include = 'object').columns.tolist()

for col in cat_cols:
  print(f"For {col} unique values are:{cleaned_data[col].unique()}",end ="\n\n")

"""It will provide a all uniques values from the categorical columns."""

# Bar plot of missing values
msno.bar(cleaned_data)

# matrix of missing values
msno.matrix(cleaned_data)

# Correlation chart of missing values
msno.heatmap(cleaned_data)



"""## Saved cleaned data"""

# save the cleaned data

cleaned_data.to_csv("Swiggy_cleaned_data.csv",index=False)

# load the cleaned data

cleaned_data_load = pd.read_csv("Swiggy_cleaned_data.csv")

# data types of cleaned data

cleaned_data_load.info()

