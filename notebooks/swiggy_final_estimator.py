# -*- coding: utf-8 -*-
"""Swiggy_Final_Estimator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13izboeUADX1YwBGhQK0K6GT7BZ7_x9g0
"""

import numpy as np
import pandas as pd

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, MinMaxScaler, PowerTransformer, OrdinalEncoder

from sklearn.model_selection import train_test_split

# Commented out IPython magic to ensure Python compatibility.
# %pip install mlflow dagshub

import dagshub
dagshub.init(repo_owner='parthgiht', repo_name='swiggy-delivery-time-prediction', mlflow=True)

import mlflow

# set the tracking server

mlflow.set_tracking_uri("https://dagshub.com/parthgiht/swiggy-delivery-time-prediction.mlflow")

# mlflow experiment

mlflow.set_experiment("Exp 6 - Final Estimator")

from sklearn import set_config

set_config(transform_output="pandas")

# load the data

Data = pd.read_csv('/content/swiggy_cleaned.csv')

Data.head()

Data['distance_type'] = pd.cut(
    Data['distance'],
    bins=[0, 5, 10, 15, 25],
    right=False,
    labels=['short', 'medium', 'long', 'very_long']
)

Data.columns

# drop columns not required for model input

columns_to_drop =  ['rider_id',
                    'restaurant_latitude',
                    'restaurant_longitude',
                    'delivery_latitude',
                    'delivery_longitude',
                    'order_date',
                    "order_time_hour",
                    "order_day",
                    "city_name",
                    "order_day_of_week",
                    "order_month"]

Data.drop(columns=columns_to_drop, inplace=True)

Data

# check for missing values

Data.isna().sum()

# check for duplicates

Data.duplicated().sum()

import missingno as msno

msno.matrix(Data)

# columns that have missing values

missing_cols = Data.isna().any(axis=0).loc[lambda x: x].index

missing_cols

"""## Drop missing values"""

temp_df = Data.copy().dropna()

# split into X and y

X = temp_df.drop(columns='time_taken')
y = temp_df['time_taken']

X

# train test split

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=42)

print("The size of train data is",X_train.shape)
print("The shape of test data is",X_test.shape)

# missing values in train data

X_train.isna().sum()

# transform target column

pt = PowerTransformer()

y_train_pt = pt.fit_transform(y_train.values.reshape(-1,1))
y_test_pt = pt.transform(y_test.values.reshape(-1,1))

"""## Pre-Processing Pipeline"""

Num_cols = ["age","ratings","pickup_time_minutes","distance"]

Nominal_cat_cols = ['weather',
                    'type_of_order',
                    'type_of_vehicle',
                    "festival",
                    "city_type",
                    "is_weekend",
                    "order_time_of_day"]

Ordinal_cat_cols = ["traffic","distance_type"]

# generate order for ordinal encoding

traffic_order = ["low","medium","high","jam"]

distance_type_order = ["short","medium","long","very_long"]

# build a preprocessor

preprocessor = ColumnTransformer(transformers=[
    ("scale", MinMaxScaler(), Num_cols),
    ("nominal_encode", OneHotEncoder(drop="first",handle_unknown="ignore",
                                     sparse_output=False), Nominal_cat_cols),
    ("ordinal_encode", OrdinalEncoder(categories=[traffic_order,distance_type_order],
                                      encoded_missing_value=-999,
                                      handle_unknown="use_encoded_value",
                                      unknown_value=-1), Ordinal_cat_cols)
],remainder="passthrough",n_jobs=-1,force_int_remainder_cols=False,verbose_feature_names_out=False)


preprocessor

# build the pipeline

processing_pipeline = Pipeline(steps=[
                                ("preprocess",preprocessor)
                            ])

processing_pipeline

# do data preprocessing

X_train_trans = processing_pipeline.fit_transform(X_train)

X_test_trans = processing_pipeline.transform(X_test)

# Commented out IPython magic to ensure Python compatibility.
# %pip install optuna

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from lightgbm import LGBMRegressor
from sklearn.ensemble import StackingRegressor
from sklearn.model_selection import cross_val_score
from sklearn.compose import TransformedTargetRegressor
from sklearn.metrics import r2_score, mean_absolute_error

# Build the best model
best_rf_params = {
    'n_estimators' : 479,
    'criterion' : 'squared_error',
    'max_depth' : 17,
    'max_features' : None,
    'min_samples_split' : 9,
    'min_samples_leaf' : 2,
    'max_samples' : 0.6603673526197067}

best_lgbm_params = {
    'n_estimators' : 154,
    'learning_rate' : 0.22234435854395157,
    'subsample' : 0.7592213724048168,
    'min_child_weight' : 20,
    'min_split_gain' : 0.004604680609280751,
    'reg_lambda' : 97.81002379097947 }



best_rf = RandomForestRegressor(**best_rf_params)
best_lgbm = LGBMRegressor(**best_lgbm_params)

lr = LinearRegression()

# Stacking regressor
stacking_reg = StackingRegressor(estimators = [("rf", best_rf),("lgbm", best_lgbm)], final_estimator = lr, cv = 5, n_jobs = -1)
# Build transformed regressor
model = TransformedTargetRegressor(regressor = stacking_reg, transformer = pt)

# train the model
model.fit(X_train_trans, y_train)

# Get the train and test prediction

y_train_pred = model.predict(X_train_trans)
y_test_pred = model.predict(X_test_trans)

# Calculate mae of train and test
train_mae = mean_absolute_error(y_train, y_train_pred)
test_mae = mean_absolute_error(y_test, y_test_pred)

# Calculate r2 score
train_r2 = r2_score(y_train, y_train_pred)
test_r2 = r2_score(y_test, y_test_pred)


# Calculate cv score
cv_scores = cross_val_score(model, X_train_trans, y_train, cv = 5, scoring = "neg_mean_absolute_error", n_jobs = -1)

-cv_scores

# Log with the mlflow
with mlflow.start_run():
  # set tags
  mlflow.set_tag("model", "Stacking Regressor")

  # log parameters
  mlflow.log_params(stacking_reg.get_params())

  # log metrics
  mlflow.log_metric("train_mae", train_mae)
  mlflow.log_metric("train_mae", test_mae)
  mlflow.log_metric("train_r2", train_r2)
  mlflow.log_metric("train_r2", test_r2)
  mlflow.log_metric("cv_score",-(cv_scores.mean()))


  # log the stacking regressor
  mlflow.sklearn.save_model(stacking_reg, "stacking_model")

